# PhD Research: Compression of Large Language Models (LLMs)

**PhD Candidate:** Ayoub Abraich  
**Institution:** Mohammed VI Polytechnic University (UM6P)  
**Advisors:** Pr. Ismail BERRADA, Pr. Imad KISSAMI  
**Start Date:** June 2025

## 🎯 Research Focus

This repository organizes my PhD research on Large Language Model compression techniques, focusing on developing novel hybrid compression strategies that combine multiple techniques for efficient deployment across different scenarios.

## 📋 Thesis Objectives

1. **Survey and benchmark classical compression methods**
   - Quantization (binary, 4-bit, mixed-precision)
   - Pruning (unstructured vs. structured)
   - Knowledge distillation
   - Matrix factorization and low-rank approximations

2. **Develop novel hybrid compression strategies** combining multiple techniques for greater efficiency

3. **Adapt and optimize strategies** for different deployment scenarios:
   - Batch or low-latency inference on GPU/TPU-based HPC clusters
   - Edge device deployment
   - Resource-constrained academic settings

4. **Design rigorous evaluation protocols** for comprehensive assessment

## 📚 Reading Progress

### Current Status
- **Papers Read:** 0
- **Papers in Queue:** 200+
- **Summary Notes:** 0
- **Last Updated:** 2025-06-20

## 🗂️ Repository Organization

- [`papers/`](papers/) - Research papers organized by topic
- [`research/`](research/) - Thesis development and experiments
- [`tools/`](tools/) - Bibliography, scripts, templates
- [`collaborations/`](collaborations/) - Collaboration materials
- [`presentations/`](presentations/) - Conference and seminar materials
- [`code/`](code/) - Implementation and experiments

## 🎯 Current Focus Areas

### This Month
- [ ] Complete survey of quantization methods (2022-2025)
- [ ] Implement baseline quantization experiments
- [ ] Draft initial research proposal

---

## 📝 Recent Updates


---
