# Master Reading List - LLM Compression

## Status Legend
- 🔴 **Not Started** - Paper not yet read
- 🟡 **In Progress** - Currently reading/taking notes
- 🟢 **Completed** - Read and summarized
- ⭐ **High Priority** - Must read for thesis
- 🎯 **Core Research** - Directly relevant to research questions

---

## 📊 Progress Overview

**Total Papers:** 200+  
**Completed:** 0  
**In Progress:** 0  
**High Priority Remaining:** 50  
**Last Updated:** 2025-06-20

---

## 🔍 Survey Papers (Start Here)

| Status | Priority | Paper | Venue | Year | File |
|--------|----------|-------|-------|------|------|
| 🔴 | ⭐ | A Survey on Model Compression for Large Language Models | TACL | 2023 | [notes](papers/07-surveys-background/notes/) |
| 🔴 | ⭐ | Efficient Large Language Models: A Survey | TMLR | 2024 | [notes](papers/07-surveys-background/notes/) |
| 🔴 | ⭐ | The Efficiency Spectrum of Large Language Models: An Algorithmic Survey | Arxiv | 2023 | [notes](papers/07-surveys-background/notes/) |

---

## 🔢 Quantization Papers

### Foundational Methods
| Status | Priority | Paper | Venue | Year | File |
|--------|----------|-------|-------|------|------|
| 🔴 | ⭐ | GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers | ICLR | 2023 | [notes](papers/01-quantization/notes/) |
| 🔴 | ⭐ | AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration | MLSys | 2024 | [notes](papers/01-quantization/notes/) |
| 🔴 | ⭐ | SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models | ICML | 2023 | [notes](papers/01-quantization/notes/) |

---

## 📖 Reading Schedule

### Week 1-2: Foundation
- [ ] Complete all survey papers
- [ ] Read 3 foundational quantization papers
- [ ] Create initial research question outline

### Week 3-4: Deep Dive - Quantization
- [ ] Read 10 quantization papers
- [ ] Implement basic quantization experiments
- [ ] Write quantization chapter outline

---

*Use the paper summary template from `tools/templates/paper-summary-template.md` for each paper*
