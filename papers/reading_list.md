# Master Reading List - LLM Compression

## Status Legend
- ğŸ”´ **Not Started** - Paper not yet read
- ğŸŸ¡ **In Progress** - Currently reading/taking notes
- ğŸŸ¢ **Completed** - Read and summarized
- â­ **High Priority** - Must read for thesis
- ğŸ¯ **Core Research** - Directly relevant to research questions

---

## ğŸ“Š Progress Overview

**Total Papers:** 200+  
**Completed:** 0  
**In Progress:** 0  
**High Priority Remaining:** 50  
**Last Updated:** 2025-06-20

---

## ğŸ” Survey Papers (Start Here)

| Status | Priority | Paper | Venue | Year | File |
|--------|----------|-------|-------|------|------|
| ğŸ”´ | â­ | A Survey on Model Compression for Large Language Models | TACL | 2023 | [notes](papers/07-surveys-background/notes/) |
| ğŸ”´ | â­ | Efficient Large Language Models: A Survey | TMLR | 2024 | [notes](papers/07-surveys-background/notes/) |
| ğŸ”´ | â­ | The Efficiency Spectrum of Large Language Models: An Algorithmic Survey | Arxiv | 2023 | [notes](papers/07-surveys-background/notes/) |

---

## ğŸ”¢ Quantization Papers

### Foundational Methods
| Status | Priority | Paper | Venue | Year | File |
|--------|----------|-------|-------|------|------|
| ğŸ”´ | â­ | GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers | ICLR | 2023 | [notes](papers/01-quantization/notes/) |
| ğŸ”´ | â­ | AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration | MLSys | 2024 | [notes](papers/01-quantization/notes/) |
| ğŸ”´ | â­ | SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models | ICML | 2023 | [notes](papers/01-quantization/notes/) |

---

## ğŸ“– Reading Schedule

### Week 1-2: Foundation
- [ ] Complete all survey papers
- [ ] Read 3 foundational quantization papers
- [ ] Create initial research question outline

### Week 3-4: Deep Dive - Quantization
- [ ] Read 10 quantization papers
- [ ] Implement basic quantization experiments
- [ ] Write quantization chapter outline

---

*Use the paper summary template from `tools/templates/paper-summary-template.md` for each paper*
