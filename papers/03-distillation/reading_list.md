# Reading List - Distillation

## Status Legend
- 🔴 **Not Started** - Paper not yet read
- 🟡 **In Progress** - Currently reading/taking notes  
- 🟢 **Completed** - Read and summarized

---

## 📊 Progress Overview

**Last Updated:** 2025-06-20

---

## 📚 Papers


| Status | Priority | Paper | Venue | Year | Link |
|--------|----------|-------|-------|------|------|
| 🔴 | ⭐ | Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence | ArXiv | 2025 | [2506.15677v1](http://arxiv.org/abs/2506.15677v1) |
| 🔴 | ⭐ | Sekai: A Video Dataset towards World Exploration | ArXiv | 2025 | [2506.15675v1](http://arxiv.org/abs/2506.15675v1) |
| 🔴 | ⭐ | deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses | ArXiv | 2025 | [2506.15648v1](http://arxiv.org/abs/2506.15648v1) |
| 🔴 | ⭐ | The AI Policy Module: Developing Computer Science Student Competency in AI Ethics and Policy | ArXiv | 2025 | [2506.15639v1](http://arxiv.org/abs/2506.15639v1) |
| 🔴 | ⭐ | Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability | ArXiv | 2025 | [2506.15629v1](http://arxiv.org/abs/2506.15629v1) |
| 🔴 | ⭐ | The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games | ArXiv | 2025 | [2506.15624v1](http://arxiv.org/abs/2506.15624v1) |
| 🔴 | ⭐ | Further Evidence for a Direct-Collapse Origin of the Supermassive Black Hole at the Center of the Infinity Galaxy | ArXiv | 2025 | [2506.15619v1](http://arxiv.org/abs/2506.15619v1) |
| 🔴 | ⭐ | The Infinity Galaxy: a Candidate Direct-Collapse Supermassive Black Hole Between Two Massive, Ringed Nuclei | ArXiv | 2025 | [2506.15618v1](http://arxiv.org/abs/2506.15618v1) |
| 🔴 | ⭐ | The Compositional Architecture of Regret in Large Language Models | ArXiv | 2025 | [2506.15617v1](http://arxiv.org/abs/2506.15617v1) |
| 🔴 | ⭐ | An efficient construction of Raz's two-source randomness extractor with improved parameters | ArXiv | 2025 | [2506.15547v1](http://arxiv.org/abs/2506.15547v1) |
| 🔴 | ⭐ | Lessons from Training Grounded LLMs with Verifiable Rewards | ArXiv | 2025 | [2506.15522v1](http://arxiv.org/abs/2506.15522v1) |
| 🔴 | ⭐ | GenRecal: Generation after Recalibration from Large to Small Vision-Language Models | ArXiv | 2025 | [2506.15681v1](http://arxiv.org/abs/2506.15681v1) |
